<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0039)http://www.nnwj.de/backpropagation.html -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Backpropagation - Neural Networks with Java</title>

<meta name="robots" content="index,follow,noarchive">
<!-- 
	This website is powered by TYPO3 - inspiring people to share!
	TYPO3 is a free open source Content Management Framework initially created by Kasper Skaarhoj and licensed under GNU/GPL.
	TYPO3 is copyright 1998-2009 of Kasper Skaarhoj. Extensions are copyright of their respective owners.
	Information and contribution at http://typo3.com/ and http://typo3.org/
-->
<!--<base href="http://www.nnwj.de/">--><base href=".">
<link rel="stylesheet" type="text/css" href="./Backpropagation - Neural Networks with Java_files/stylesheet_b31b591750.css">
<link rel="stylesheet" type="text/css" href="./Backpropagation - Neural Networks with Java_files/stylesheet.css">
<script async="" src="./Backpropagation - Neural Networks with Java_files/analytics.js"></script><script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-429779-8', 'nnwj.de');
  ga('set', 'anonymizeIp', true);
  ga('send', 'pageview');
</script>
<meta name="language" content="en">
<script type="text/javascript" src="./Backpropagation - Neural Networks with Java_files/javascript_f52d1a094f.js"></script>



</head>
<body cz-shortcut-listen="true">

<div id="CONTENT"><div id="breadcrumb"><a href="http://www.nnwj.de/" target="_top" rel="nofollow" title="Back to the homepage">Homepage</a>&nbsp;«&nbsp;<a href="http://www.nnwj.de/neural-net-overview.html" title="Neural Net Overview">Neural Net Overview</a>&nbsp;«&nbsp;<a href="http://www.nnwj.de/learning-process.html" title="The Learning Process">The Learning Process</a>&nbsp;</div><h1>Backpropagation</h1><!--INTLINKS_begin--><div style="float:right;margin:8px 0 8px 8px;"><script type="text/javascript"><!--
google_ad_client = "pub-0985808748422360";
google_ad_width = 200;
google_ad_height = 200;
google_ad_format = "200x200_as";
google_ad_type = "text_image";
google_ad_channel = "";
google_color_border = "dddddd";
google_color_bg = "ffffff";
google_color_link = "330033";
google_color_text = "666666";
google_color_url = "888888";
//-->
</script>
<script type="text/javascript" src="./Backpropagation - Neural Networks with Java_files/show_ads.js">
</script><br>Werbung</div><p>Backpropagation is a supervised learning algorithm and is mainly used by Multi-Layer-Perceptrons to change the weights connected to the net's hidden neuron layer(s).
</p>
<p>&nbsp;</p>
<p>The backpropagation algorithm uses a computed output error to change the weight values in backward direction.
</p>
<p>To get this net error, a <a href="http://www.nnwj.de/forwardpropagation.html" title="forwardpropagation">forwardpropagation</a> phase must have been done before. While propagating in forward direction, the neurons are being activated using the sigmoid activation function.</p>The formula of <b>sigmoid activation</b> is:
<pre class="small">               1
    f(x) = ---------
           1 + e<sup>-input</sup>
</pre><p>The algorithm works as follows:</p><ol><li>Perform the forwardpropagation phase for an input pattern and calculate the output error
</li><li>Change all weight values of each weight matrix using the formula
weight(old) + learning rate * output error * output(neurons i) * output(neurons i+1) * ( 1 - output(neurons i+1) ) 
</li><li>Go to step 1
</li><li>The algorithm ends, if all output patterns match their target patterns 
</li></ol><p>Example:
</p>
<p>Suppose you have the following 3-layered Multi-Layer-Perceptron:</p><div class="csc-textpic csc-textpic-center csc-textpic-below csc-textpic-caption-c"><div class="csc-textpic-imagewrap" style="width:234px;"><dl class="csc-textpic-image csc-textpic-firstcol csc-textpic-lastcol" style="width:234px;"><dt><img src="./Backpropagation - Neural Networks with Java_files/1_3-backpropagation.gif" width="234" height="320" border="0" alt="Backpropagation in a 3-layered Multi-Layer-Perceptron" title="Backpropagation in a 3-layered Multi-Layer-Perceptron"></dt><dd class="csc-textpic-caption">Backpropagation in a 3-layered Multi-Layer-Perceptron</dd></dl></div></div><div class="csc-textpic-clear"><!-- --></div><img src="./Backpropagation - Neural Networks with Java_files/clear.gif" width="1" height="10" border="0" class="spacer-gif" alt="" title=""><br><p>Patterns to be learned:</p>
				<table cellspacing="2" cellpadding="1" class="contenttable contenttable-0">
					<tbody><tr class="tr-even tr-0">
							<td class="td-0">input</td>
							<td class="td-last td-1">target</td>
					</tr>
					<tr class="tr-odd tr-1">
							<td class="td-0">0 1</td>
							<td class="td-last td-1">0</td>
					</tr>
					<tr class="tr-even tr-last">
							<td class="td-0">1 1</td>
							<td class="td-last td-1">1</td>
					</tr>
				</tbody></table><img src="./Backpropagation - Neural Networks with Java_files/clear.gif" width="1" height="10" border="0" class="spacer-gif" alt="" title=""><br><p>First, the weight values are set to random values: 0.62, 0.42, 0.55, -0.17 for weight matrix 1 and 0.35, 0.81 for weight matrix 2. 
</p>
<p>The learning rate of the net is set to 0.25. 
</p>
<p>Next, the values of the first input pattern (0 1) are set to the neurons of the input layer (the output of the input layer is the same as its input).
</p>
<p>&nbsp;</p>
<p>The neurons in the hidden layer are activated:</p><pre class="small">Input of hidden neuron 1:       0 * 0.62 + 1 * 0.55    = 0.55
Input of hidden neuron 2:       0 * 0.42 + 1 * (-0.17) = -0.17
Output of hidden neuron 1:      1 / ( 1 + exp(-0.55) ) = 0.634135591
Output of hidden neuron 2:      1 / ( 1 + exp(+0.17) ) = 0.457602059
</pre><p>The neurons in the output layer are activated:</p><pre class="small">Input of output neuron:         0.634135591 * 0.35 + 0.457602059 * 0.81 = 0.592605124
Output of output neuron:        1 / ( 1 + exp(-0.592605124) ) = 0.643962658
Compute an error value by
subtracting output from target: 0 - 0.643962658 = <b>-0.643962658</b>
</pre><p>Now that we got the output error, let's do the backpropagation. 
</p>
<p>We start with changing the weights in weight matrix 2:</p><pre class="small">Value for changing weight 1:    0.25 * (-0.643962658) * 0.634135591
                                * 0.643962658 * (1-0.643962658) = -0.023406638
Value for changing weight 2:    0.25 * (-0.643962658) * 0.457602059
                                * 0.643962658 * (1-0.643962658) = -0.016890593
Change weight 1:                0.35 + (-0.023406638) = 0.326593362
Change weight 2:                0.81 + (-0.016890593) = 0.793109407
</pre><p>Now we will change the weights in weight matrix 1:</p><pre class="small">Value for changing weight 1:    0.25 * (-0.643962658) * 0
                                * 0.634135591 * (1-0.634135591) = 0
Value for changing weight 2:    0.25 * (-0.643962658) * 0
                                * 0.457602059 * (1-0.457602059) = 0
Value for changing weight 3:    0.25 * (-0.643962658) * 1
                                * 0.634135591 * (1-0.634135591) = -0.037351064
Value for changing weight 4:    0.25 * (-0.643962658) * 1
                                * 0.457602059 * (1-0.457602059) = -0.039958271
Change weight 1:                0.62 + 0 = 0.62         (not changed)
Change weight 2:                0.42 + 0 = 0.42         (not changed)
Change weight 3:                0.55 + (-0.037351064) = 0.512648936
Change weight 4:                -0.17+ (-0.039958271) = -0.209958271
</pre><div style="float:right;margin:8px 0 8px 8px;"><script type="text/javascript"><!--
google_ad_client = "pub-0985808748422360";
google_ad_width = 200;
google_ad_height = 200;
google_ad_format = "200x200_as";
google_ad_type = "text_image";
google_ad_channel = "";
google_color_border = "dddddd";
google_color_bg = "ffffff";
google_color_link = "330033";
google_color_text = "666666";
google_color_url = "888888";
//-->
</script>
<script type="text/javascript" src="./Backpropagation - Neural Networks with Java_files/show_ads.js">
</script><br>Werbung</div><p>The first input pattern had been propagated through the net. 
</p>
<p>The same procedure is used for the next input pattern, but then with the changed weight values. 
</p>
<p>After the forward and backward propagation of the second pattern, one learning step is complete and the net error can be calculated by adding up the squared output errors of each pattern. 
</p>
<p>By performing this procedure repeatedly, this error value gets smaller and smaller. 
</p>
<p>&nbsp;</p>
<p>The algorithm is successfully finished, if the net error is zero (perfect) or approximately zero.
</p>
<p>&nbsp;</p>
<p>Note that this algorithm is also applicable for Multi-Layer-Perceptrons with more than one hidden layer. </p><img src="./Backpropagation - Neural Networks with Java_files/clear.gif" width="1" height="20" border="0" class="spacer-gif" alt="" title=""><br><div class="csc-header csc-header-n18"><h3>"What happens, if all values of an input pattern are zero?"</h3><img src="./Backpropagation - Neural Networks with Java_files/clear.gif" width="1" height="5" border="0" class="spacer-gif" alt="" title=""><br></div><p>If all values of an input pattern are zero, the weights in weight matrix 1 would never be changed for this pattern and the net could not learn it. Due to that fact, a "pseudo input" is created, called Bias that has a constant output value of 1.
</p>
<p>&nbsp;</p>
<p>This changes the structure of the net in the following way:</p><div class="csc-textpic csc-textpic-center csc-textpic-below csc-textpic-caption-c"><div class="csc-textpic-imagewrap" style="width:302px;"><dl class="csc-textpic-image csc-textpic-firstcol csc-textpic-lastcol" style="width:302px;"><dt><img src="./Backpropagation - Neural Networks with Java_files/1_3-mlp-biased.gif" width="302" height="320" border="0" alt="Backpropagation in a 3-layered Multi-Layer-Perceptron using Bias values" title="Backpropagation in a 3-layered Multi-Layer-Perceptron using Bias values"></dt><dd class="csc-textpic-caption">Backpropagation in a 3-layered <a href="http://www.nnwj.de/multi-layer-perceptron.html" title="Multi-Layer-Perceptron">Multi-Layer-Perceptron</a> using Bias values</dd></dl></div></div><div class="csc-textpic-clear"><!-- --></div><p>These additional weights, leading to the neurons of the hidden layer and the output layer, have initial random values and are changed in the same way as the other weights. By sending a constant output of 1 to following neurons, it is guaranteed that the input values of those neurons are always differing from zero.</p><!--INTLINKS_end--><div style="margin:20px 0"><iframe src="./Backpropagation - Neural Networks with Java_files/like.html" scrolling="no" frameborder="0" allowtransparency="true" style="border:none;overflow:hidden;width:250px;height:40px;"></iframe></div><div align="center"><div id="RIGHT_NAV"><div style="border:1px solid #666;"><div class="RIGHT_NAV_HEAD">Sub-sections</div><div class="menuitem"><a href="http://www.nnwj.de/learning-neural-nets.html" title="What does &quot;learning&quot; mean refering to neural nets?">Learning and NNs</a></div><div class="menuitem"><a href="http://www.nnwj.de/supervised-unsupervised.html" title="Supervised and unsupervised learning">Supervised / Unsupervised</a></div><div class="menuitem"><a href="http://www.nnwj.de/forwardpropagation.html" title="Forwardpropagation">Forwardpropagation</a></div><div class="menuitem-act"><a href="./Backpropagation - Neural Networks with Java_files/Backpropagation - Neural Networks with Java.html" title="Backpropagation">Backpropagation</a></div><div class="menuitem"><a href="http://www.nnwj.de/selforganization.html" title="Selforganization">Selforganization</a></div></div></div></div><div id="PAGE_FOOTER">Copyright (c) 1996-2015&nbsp;Neural Networks with Java - Jochen Fröhlich. All rights reserved.</div></div>
<div id="MENU"><div style="margin:0;padding:0 0 15px 0;">
<form style="margin:0;padding:0;" action="http://www.nnwj.de/cse.html" id="searchbox_002051702349312966734:eaqknyc6hse">
<span style="font-size:9px;font-weight:700;color:#000;">Search</span><br>
    <input type="hidden" name="cx" value="002051702349312966734:eaqknyc6hse">
    <input type="hidden" name="cof" value="FORID:11">
    <input type="hidden" name="ie" value="UTF-8">
    <input type="text" name="q" size="20" id="search-box" style="width:140px;margin:0;padding:2px;border:1px solid #881166;font-size:9px;"><input type="submit" name="sa" value="&gt;" style="width:18px;margin:0 0 0 2px;padding:1px;border:1px solid #881166;font-size:9px;">
</form>
</div>
<div id="MAIN_MENU"><div class="menuitem-1"><a href="http://www.nnwj.de/contents.html" title="Contents">::&nbsp;CONTENTS</a></div><div class="menuitem-1"><a href="http://www.nnwj.de/preface.html" title="Preface">::&nbsp;PREFACE</a></div><div class="menuitem-1-act"><a href="http://www.nnwj.de/neural-net-overview.html" title="Neural Net Overview">::&nbsp;NEURAL NET OVERVIEW</a></div><div class="menuitem-2"><a href="http://www.nnwj.de/neural-nets-explained.html" title="What Are Neural Nets?">·&nbsp;What Are Neural Nets?</a></div><div class="menuitem-2"><a href="http://www.nnwj.de/neural-net-types.html" title="Types of Neural Nets">·&nbsp;Types of Neural Nets</a></div><div class="menuitem-2-act"><a href="http://www.nnwj.de/learning-process.html" title="The Learning Process"><b>·</b>&nbsp;The Learning Process</a></div><div class="menuitem-1"><a href="http://www.nnwj.de/class-structure.html" title="Class Structure">::&nbsp;CLASS STRUCTURE</a></div><div class="menuitem-1"><a href="http://www.nnwj.de/using-the-classes.html" title="Using the Classes">::&nbsp;USING THE CLASSES</a></div><div class="menuitem-1"><a href="http://www.nnwj.de/sample-applet.html" title="3D Travelling Salesman Problem Applet using SOM">::&nbsp;SAMPLE APPLET</a></div><div class="menuitem-1"><a href="http://www.nnwj.de/glossary.html" title="Glossary">::&nbsp;GLOSSARY</a></div><div class="menuitem-1"><a href="http://www.nnwj.de/literature.html" title="Literature">::&nbsp;LITERATURE</a></div><div id="GLOBAL_NAV"><div class="menuitem"><a href="http://www.nnwj.de/contact.html" title="Contact / Imprint">::&nbsp;Contact / Imprint</a></div><div class="menuitem"><a href="http://www.nnwj.de/download.html" title="Download">::&nbsp;Download</a></div><div class="menuitem"><a href="http://www.nnwj.de/" title="Neural Networks with Java Homepage" target="_top">::&nbsp;Homepage</a></div></div></div></div>
<div id="PAGE_HEADER"><a style="text-decoration:none;" href="http://www.nnwj.de/"><img src="./Backpropagation - Neural Networks with Java_files/nnwj-2004-edition.jpg" width="500" height="84" border="0" alt="Neural Networks with Java" title="Neural Networks with Java"></a></div>
<div id="PAGE_HEADER_TOPIC"><img src="./Backpropagation - Neural Networks with Java_files/header-topic.jpg" width="700" height="60" border="0" alt="Neural Net Components in an Object Oriented Class Structure" title="Neural Net Components in an Object Oriented Class Structure"></div>


</body><iframe allowtransparency="true" frameborder="0" id="abs-top-frame" src="./Backpropagation - Neural Networks with Java_files/top.html" style="position: fixed; z-index: 4294967295; overflow: hidden; top: 0px; left: 0px; right: 0px; width: 138px; height: 13px; max-height: none; min-height: 0px; margin: 0px auto; padding: 0px; border: 0px; display: block; background-color: transparent;"></iframe></html>